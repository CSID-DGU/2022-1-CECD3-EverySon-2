{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sj/miniconda3/envs/everyson/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModel, MarianMTModel, M2M100ForConditionalGeneration\n",
    "import tqdm\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# nllb\n",
    "def _eng2kor(text, model, tokenizer, device):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    translated_tokens = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"kor_Hang\"])\n",
    "    return tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\n",
    "\n",
    "# opus-mt\n",
    "def __eng2kor(text, model, tokenizer, device):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    translated_tokens = model.generate(**inputs)\n",
    "    return tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\n",
    "\n",
    "# m2m100\n",
    "def eng2kor(text, model, tokenizer, device):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    translated_tokens = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.get_lang_id(\"ko\"))\n",
    "    return tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\n",
    "\n",
    "def load_model(model_name):\n",
    "    # model = AutoModelForSeq2SeqLM.from_pretrained(model_name) # nllb\n",
    "    # model = MarianMTModel.from_pretrained(model_name) # opus-mt\n",
    "    model = M2M100ForConditionalGeneration.from_pretrained(model_name) # m2m100\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    return model, tokenizer, device\n",
    "\n",
    "def Generate_KR(txt_file: str, model_name: str, data_type: str) -> None:\n",
    "    \"\"\"Generate Korean text from English text using pretrained model from huggingface.\n",
    "\n",
    "    Args:\n",
    "        txt_file (str): path to txt file to translate.\n",
    "        model_name (str): name of pretrained model from huggingface.\n",
    "        data_type (str): type of data to translate. ['MELD', 'EMORY', 'IEMOCAP', 'DD']\n",
    "    \"\"\"\n",
    "    print(\"*** Load model start ***\")\n",
    "    model, tokenizer, device = load_model(model_name)\n",
    "    print(f\"*   device : {device}   *\")\n",
    "    print(f\"*   model : {model_name}   *\")\n",
    "    \n",
    "    print(\"*** Read file start ***\")\n",
    "    print(f\"*   data_type : {data_type}   *\")\n",
    "    print(f\"*   txt_file : {txt_file}   *\")\n",
    "    f = open(txt_file, 'r')\n",
    "    dataset = f.readlines()\n",
    "    f.close()\n",
    "    print(\"*   total \" + str(len(dataset)) + \" lines   *\")\n",
    "    \n",
    "    print(\"*** write file start ***\")\n",
    "    base_file = txt_file.split('_')\n",
    "    ko_txt_file = base_file[0] + '_' + model_name.replace('/', '-') + '_' + base_file[1][:-4] + '_ko.txt' \n",
    "    print(f\"*   write_file : {ko_txt_file}   *\")\n",
    "    \n",
    "    with open(ko_txt_file, 'w') as f:\n",
    "        for i, data in tqdm.tqdm(enumerate(dataset), total=len(dataset)):\n",
    "            if data == '\\n':\n",
    "                f.write('\\n')\n",
    "            else:\n",
    "                if data_type == 'MELD':\n",
    "                    if i <= 1:\n",
    "                        continue\n",
    "                    # emotion: 'anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise'\n",
    "                    # sentiment: {'positive': [\"joy\"], 'negative': [\"anger\", \"disgust\", \"fear\", \"sadness\"], 'neutral': [\"neutral\", \"surprise\"]}\n",
    "                    speaker, utt, emo, senti = data.strip().split('\\t')\n",
    "                    emodict = {'anger': \"anger\", 'disgust': \"disgust\", 'fear': \"fear\", 'joy': \"joy\", 'neutral': \"neutral\", 'sadness': \"sad\", 'surprise': 'surprise'}\n",
    "                    emo = emodict[emo]\n",
    "                    utt = eng2kor(utt, model, tokenizer, device)[0]\n",
    "                    out = speaker + '\\t' + utt + '\\t' + emo + '\\t' + senti + '\\n'\n",
    "                    f.write(out)\n",
    "                elif data_type == 'EMORY':\n",
    "                    speaker = data.strip().split('\\t')[0]\n",
    "                    utt = ' '.join(data.strip().split('\\t')[1:-1])\n",
    "                    emo = data.strip().split('\\t')[-1]\n",
    "                    emodict = {'Joyful': \"joy\", 'Mad': \"anger\", 'Peaceful': \"neutral\", 'Powerful': \"surprise\", 'Neutral': \"neutral\", 'Sad': \"sad\", 'Scared': 'fear'}\n",
    "                    emo = emodict[emo]\n",
    "                    sentidict = {\"joy\": \"positive\", \"anger\": \"negative\", \"disgust\": \"negative\", \"fear\": \"negative\", \"sad\": \"negative\", \"neutral\": \"neutral\", \"surprise\": \"neutral\"}\n",
    "                    senti = sentidict[emo]\n",
    "                    utt = eng2kor(utt, model, tokenizer, device)[0]\n",
    "                    out = speaker + '\\t' + utt + '\\t' + emo + '\\t' + senti + '\\n'\n",
    "                    f.write(out)\n",
    "                elif data_type == 'IEMOCAP':\n",
    "                    speaker = data.strip().split('\\t')[0]\n",
    "                    utt = ' '.join(data.strip().split('\\t')[1:-1])\n",
    "                    emo = data.strip().split('\\t')[-1]\n",
    "                    emodict = {'ang': \"anger\", 'hap': \"joy\", 'neu': \"neutral\", 'sad': \"sad\", 'exc': \"surprise\", 'fru': \"disgust\", 'fea': \"fear\"}\n",
    "                    emo = emodict[emo]\n",
    "                    sentidict = {\"joy\": \"positive\", \"anger\": \"negative\", \"disgust\": \"negative\", \"fear\": \"negative\", \"sad\": \"negative\", \"neutral\": \"neutral\", \"surprise\": \"neutral\"}\n",
    "                    senti = sentidict[emo]\n",
    "                    utt = eng2kor(utt, model, tokenizer, device)[0]\n",
    "                    out = speaker + '\\t' + utt + '\\t' + emo + '\\t' + senti + '\\n'\n",
    "                    f.write(out)\n",
    "                elif data_type == 'DD':\n",
    "                    speaker, utt, emo = data.strip().split('\\t')\n",
    "                    emodict = {'anger': \"anger\", 'disgust': \"disgust\", 'fear': \"fear\", 'happiness': \"joy\", 'neutral': \"neutral\", 'sadness': \"sad\", 'surprise': \"surprise\"}\n",
    "                    emo = emodict[emo]\n",
    "                    sentidict = {\"joy\": \"positive\", \"anger\": \"negative\", \"disgust\": \"negative\", \"fear\": \"negative\", \"sad\": \"negative\", \"neutral\": \"neutral\", \"surprise\": \"neutral\"}\n",
    "                    senti = sentidict[emo]\n",
    "                    utt = eng2kor(utt, model, tokenizer, device)[0]\n",
    "                    out = speaker + '\\t' + utt + '\\t' + emo + '\\t' + senti + '\\n'\n",
    "                    f.write(out)\n",
    "                else:\n",
    "                    print(\"* unknown data_type *\")\n",
    "                    break\n",
    "\n",
    "# in facebook/nllb-200 some errors in translation\n",
    "model_name = \"facebook/nllb-200-1.3B\"\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "model_name = \"Helsinki-NLP/opus-mt-tc-big-en-ko\"\n",
    "model_name = \"facebook/m2m100_1.2B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Load model start ***\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 11.77 GiB total capacity; 10.81 GiB already allocated; 5.50 MiB free; 10.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Generate_KR(\u001b[39m'\u001b[39;49m\u001b[39m../data/MELD/multi/MELD_dev.txt\u001b[39;49m\u001b[39m'\u001b[39;49m, model_name, \u001b[39m'\u001b[39;49m\u001b[39mMELD\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [15], line 46\u001b[0m, in \u001b[0;36mGenerate_KR\u001b[0;34m(txt_file, model_name, data_type)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m\"\"\"Generate Korean text from English text using pretrained model from huggingface.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39m    data_type (str): type of data to translate. ['MELD', 'EMORY', 'IEMOCAP', 'DD']\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m*** Load model start ***\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m model, tokenizer, device \u001b[39m=\u001b[39m load_model(model_name)\n\u001b[1;32m     47\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*   device : \u001b[39m\u001b[39m{\u001b[39;00mdevice\u001b[39m}\u001b[39;00m\u001b[39m   *\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*   model : \u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m   *\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [15], line 34\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     32\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m     33\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     35\u001b[0m \u001b[39mreturn\u001b[39;00m model, tokenizer, device\n",
      "File \u001b[0;32m~/miniconda3/envs/everyson/lib/python3.10/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/miniconda3/envs/everyson/lib/python3.10/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/everyson/lib/python3.10/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 579 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/everyson/lib/python3.10/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/everyson/lib/python3.10/site-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    603\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    604\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/everyson/lib/python3.10/site-packages/torch/nn/modules/module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    923\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 925\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 11.77 GiB total capacity; 10.81 GiB already allocated; 5.50 MiB free; 10.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "Generate_KR('../data/MELD/multi/MELD_dev.txt', model_name, 'MELD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Load model start ***\n",
      "*   device : cuda   *\n",
      "*** Read file start ***\n",
      "*   data_type : MELD   *\n",
      "*   txt_file : ../data/MELD/multi/MELD_test.txt   *\n",
      "*   total 2891 lines   *\n",
      "*** write file start ***\n",
      "*   write_file : ../data/MELD/multi/MELD_facebook-nllb-200-1.3B_test_ko.txt   *\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2891/2891 [07:36<00:00,  6.34it/s]\n"
     ]
    }
   ],
   "source": [
    "Generate_KR('../data/MELD/multi/MELD_test.txt', model_name, 'MELD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Load model start ***\n",
      "*   device : cuda   *\n",
      "*   model : facebook/m2m100_1.2B   *\n",
      "*** Read file start ***\n",
      "*   data_type : MELD   *\n",
      "*   txt_file : ../data/MELD/multi/MELD_train.txt   *\n",
      "*   total 11028 lines   *\n",
      "*** write file start ***\n",
      "*   write_file : ../data/MELD/multi/MELD_facebook-m2m100_1.2B_train_ko.txt   *\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11028 [00:00<?, ?it/s]/home/sj/miniconda3/envs/everyson/lib/python3.10/site-packages/transformers/generation_utils.py:1296: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 200 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 11028/11028 [48:05<00:00,  3.82it/s] \n"
     ]
    }
   ],
   "source": [
    "Generate_KR('../data/MELD/multi/MELD_train.txt', model_name, 'MELD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Load model start ***\n",
      "** device : cuda **\n",
      "*** Read file start ***\n",
      "** data_type : EMORY **\n",
      "** txt_file : ../data/EMORY/EMORY_dev.txt **\n",
      "**  total 1442 lines **\n",
      "*** write file start ***\n",
      "** write_file : ../data/EMORY/EMORY_dev_facebook-nllb-200-1.3B_ko.txt **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1442/1442 [04:22<00:00,  5.49it/s]\n"
     ]
    }
   ],
   "source": [
    "Generate_KR('../data/EMORY/EMORY_dev.txt', model_name, 'EMORY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Load model start ***\n",
      "** device : cuda **\n",
      "*** Read file start ***\n",
      "** data_type : EMORY **\n",
      "** txt_file : ../data/EMORY/EMORY_test.txt **\n",
      "**  total 1412 lines **\n",
      "*** write file start ***\n",
      "** write_file : ../data/EMORY/EMORY_test_facebook-nllb-200-1.3B_ko.txt **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1412/1412 [04:48<00:00,  4.89it/s]\n"
     ]
    }
   ],
   "source": [
    "Generate_KR('../data/EMORY/EMORY_test.txt', model_name, 'EMORY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Load model start ***\n",
      "** device : cuda **\n",
      "*** Read file start ***\n",
      "** data_type : EMORY **\n",
      "** txt_file : ../data/EMORY/EMORY_train.txt **\n",
      "**  total 10646 lines **\n",
      "*** write file start ***\n",
      "** write_file : ../data/EMORY/EMORY_train_facebook-nllb-200-1.3B_ko.txt **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10646/10646 [34:01<00:00,  5.22it/s] \n"
     ]
    }
   ],
   "source": [
    "Generate_KR('../data/EMORY/EMORY_train.txt', model_name, 'EMORY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Load model start ***\n",
      "** device : cuda **\n",
      "*** Read file start ***\n",
      "** data_type : IEMOCAP **\n",
      "** txt_file : ../data/iemocap/iemocap_dev.txt **\n",
      "**  total 658 lines **\n",
      "*** write file start ***\n",
      "** write_file : ../data/iemocap/iemocap_facebook-nllb-200-1.3B_dev_ko.txt **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/658 [00:00<?, ?it/s]/home/sj/miniconda3/envs/everyson/lib/python3.10/site-packages/transformers/generation_utils.py:1296: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 200 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 658/658 [01:53<00:00,  5.81it/s]\n"
     ]
    }
   ],
   "source": [
    "Generate_KR('../data/iemocap/iemocap_dev.txt', model_name, 'IEMOCAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Load model start ***\n",
      "** device : cuda **\n",
      "*** Read file start ***\n",
      "** data_type : IEMOCAP **\n",
      "** txt_file : ../data/iemocap/iemocap_test.txt **\n",
      "**  total 1653 lines **\n",
      "*** write file start ***\n",
      "** write_file : ../data/iemocap/iemocap_facebook-nllb-200-1.3B_test_ko.txt **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1653/1653 [05:12<00:00,  5.29it/s]\n"
     ]
    }
   ],
   "source": [
    "Generate_KR('../data/iemocap/iemocap_test.txt', model_name, 'IEMOCAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Load model start ***\n",
      "** device : cuda **\n",
      "*** Read file start ***\n",
      "** data_type : IEMOCAP **\n",
      "** txt_file : ../data/iemocap/iemocap_train.txt **\n",
      "**  total 5270 lines **\n",
      "*** write file start ***\n",
      "** write_file : ../data/iemocap/iemocap_facebook-nllb-200-1.3B_train_ko.txt **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5270/5270 [16:24<00:00,  5.35it/s]  \n"
     ]
    }
   ],
   "source": [
    "Generate_KR('../data/iemocap/iemocap_train.txt', model_name, 'IEMOCAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Load model start ***\n",
      "** device : cuda **\n",
      "*** Read file start ***\n",
      "** data_type : DD **\n",
      "** txt_file : ../data/dailydialog/dailydialog_dev.txt **\n",
      "**  total 9068 lines **\n",
      "*** write file start ***\n",
      "** write_file : ../data/dailydialog/dailydialog_facebook-nllb-200-1.3B_dev_ko.txt **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9068/9068 [24:09<00:00,  6.26it/s]  \n"
     ]
    }
   ],
   "source": [
    "Generate_KR('../data/dailydialog/dailydialog_dev.txt', model_name, 'DD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Load model start ***\n",
      "*   device : cuda   *\n",
      "*** Read file start ***\n",
      "*   data_type : DD   *\n",
      "*   txt_file : ../data/dailydialog/dailydialog_test.txt   *\n",
      "*   total 8739 lines   *\n",
      "*** write file start ***\n",
      "*   write_file : ../data/dailydialog/dailydialog_facebook-nllb-200-1.3B_test_ko.txt   *\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8739/8739 [23:48<00:00,  6.12it/s]  \n"
     ]
    }
   ],
   "source": [
    "Generate_KR('../data/dailydialog/dailydialog_test.txt', model_name, 'DD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Load model start ***\n",
      "*   device : cuda   *\n",
      "*** Read file start ***\n",
      "*   data_type : DD   *\n",
      "*   txt_file : ../data/dailydialog/dailydialog_train.txt   *\n",
      "*   total 98287 lines   *\n",
      "*** write file start ***\n",
      "*   write_file : ../data/dailydialog/dailydialog_facebook-nllb-200-1.3B_train_ko.txt   *\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98287/98287 [4:25:14<00:00,  6.18it/s]   \n"
     ]
    }
   ],
   "source": [
    "Generate_KR('../data/dailydialog/dailydialog_train.txt', model_name, 'DD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "def cat_files(path: str, out_file: str) -> None:\n",
    "    \"\"\"Concat all files in path to out_file\n",
    "\n",
    "    Args:\n",
    "        path (str): path to directory containing files to concatenate\n",
    "        out_file (str): output file\n",
    "    \"\"\"\n",
    "    print(f\"*** Concatenating files ***\\n*   in {path}   *\\n*   to {out_file}   *\")\n",
    "    # create parent directory if not exists\n",
    "    out_path = Path(out_file)\n",
    "    out_dir = out_path.parent\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    files = glob.glob(path, recursive=True)\n",
    "    if out_file in files: files.remove(out_file)\n",
    "    print(f\"*** Found {len(files)} files ***\")\n",
    "    if len(files) <= 30:\n",
    "        for f in files:\n",
    "            print(f\"*   {f}   *\")\n",
    "            \n",
    "    with open(out_path, 'w') as outfile:\n",
    "        for fname in files:\n",
    "            with open(fname, 'r') as infile:\n",
    "                shutil.copyfileobj(infile, outfile)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Concatenating files ***\n",
      "*   in ../data/**/*_facebook-nllb-200-1.3B_dev_ko.txt   *\n",
      "*   to ../data/EMOTION/multi/full_facebook-nllb-200-1.3B_dev_ko.txt   *\n",
      "*** Found 4 files ***\n",
      "*   ../data/EMORY/EMORY_facebook-nllb-200-1.3B_dev_ko.txt   *\n",
      "*   ../data/MELD/multi/MELD_facebook-nllb-200-1.3B_dev_ko.txt   *\n",
      "*   ../data/iemocap/iemocap_facebook-nllb-200-1.3B_dev_ko.txt   *\n",
      "*   ../data/dailydialog/dailydialog_facebook-nllb-200-1.3B_dev_ko.txt   *\n"
     ]
    }
   ],
   "source": [
    "cat_files(\"../data/**/*_facebook-nllb-200-1.3B_dev_ko.txt\", \"../data/EMOTION/multi/full_facebook-nllb-200-1.3B_dev_ko.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Concatenating files ***\n",
      "*   in ../data/**/*_facebook-nllb-200-1.3B_test_ko.txt   *\n",
      "*   to ../data/EMOTION/multi/full_facebook-nllb-200-1.3B_test_ko.txt   *\n",
      "*** Found 4 files ***\n",
      "*   ../data/EMORY/EMORY_facebook-nllb-200-1.3B_test_ko.txt   *\n",
      "*   ../data/MELD/multi/MELD_facebook-nllb-200-1.3B_test_ko.txt   *\n",
      "*   ../data/iemocap/iemocap_facebook-nllb-200-1.3B_test_ko.txt   *\n",
      "*   ../data/dailydialog/dailydialog_facebook-nllb-200-1.3B_test_ko.txt   *\n"
     ]
    }
   ],
   "source": [
    "cat_files(\"../data/**/*_facebook-nllb-200-1.3B_test_ko.txt\", \"../data/EMOTION/multi/full_facebook-nllb-200-1.3B_test_ko.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Concatenating files ***\n",
      "*   in ../data/**/*_facebook-nllb-200-1.3B_train_ko.txt   *\n",
      "*   to ../data/EMOTION/multi/full_facebook-nllb-200-1.3B_train_ko.txt   *\n",
      "*** Found 4 files ***\n",
      "*   ../data/EMORY/EMORY_facebook-nllb-200-1.3B_train_ko.txt   *\n",
      "*   ../data/MELD/multi/MELD_facebook-nllb-200-1.3B_train_ko.txt   *\n",
      "*   ../data/iemocap/iemocap_facebook-nllb-200-1.3B_train_ko.txt   *\n",
      "*   ../data/dailydialog/dailydialog_facebook-nllb-200-1.3B_train_ko.txt   *\n"
     ]
    }
   ],
   "source": [
    "cat_files(\"../data/**/*_facebook-nllb-200-1.3B_train_ko.txt\", \"../data/EMOTION/multi/full_facebook-nllb-200-1.3B_train_ko.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('everyson')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d567a2550442f9dd0c45e5ead528636f20f97129ebaac657d8a245afe575c6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
